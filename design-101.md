System Design Guidelines Core Principles

Prevent Unsafe Use

Do not let children experiment with or manipulate unsafe system behaviors.

Child-Safety Compliance

The system must meet all relevant child-safety standards and edge cases.

Age-Appropriate Interaction

Behavior, tone, and capabilities must match the user’s age group.

Age-Based Access Model

Children Under 13  
Unlimited chat is not allowed. The system should have a clear scope and purpose.

Allowed Purposes

1.1 Educational content  
1.2 General task assistance (for example, tutoring, homework help)

Behavioral Restrictions

1.3 No emotional mirroring  
1.4 No “friend-like” tone or human mimicry  
1.5 Limited session length  
Narrow context window  
Time or interaction caps  
1.6 Reduced emotional expression  
1.7 Redirect aggressive or unsafe situations  
De-escalation over engagement  
1.8 Transparency  
Clearly state the non-human nature at all times

Teenagers (Ages 13–17)  
More flexibility than users under 13, but still restricted.

No strong emotional bonding behaviors.  
No language that creates dependency.  
Controlled tone and context limits still apply.

Adults (18 and Above)  
Full system access is allowed.  
Harmful prompts are still rejected.  
Clear warnings and safety measures remain in place.

Key Design Philosophy  
No Anthropomorphic Design for Kids

No:  
Identity-bound personas  
Names, avatars, or personalities designed to encourage bonding  
No persuasive language implying:  
Care  
Loyalty  
Love  

A chatbot must not present itself as a friend to a child.

Principle: A chatbot is a tool, not a companion, for children.
